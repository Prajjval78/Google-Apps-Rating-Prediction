{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10418138,"sourceType":"datasetVersion","datasetId":6456806},{"sourceId":10467582,"sourceType":"datasetVersion","datasetId":6480997},{"sourceId":10467667,"sourceType":"datasetVersion","datasetId":6481060},{"sourceId":10489020,"sourceType":"datasetVersion","datasetId":6494289},{"sourceId":10633365,"sourceType":"datasetVersion","datasetId":6583437}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport math\n\nimport networkx as nx\n#from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\n\n#d='cuda:1'\nd='cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.480872Z","iopub.execute_input":"2025-02-18T07:25:33.481201Z","iopub.status.idle":"2025-02-18T07:25:33.485815Z","shell.execute_reply.started":"2025-02-18T07:25:33.481176Z","shell.execute_reply":"2025-02-18T07:25:33.484613Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#pip install -r /kaggle/working/ImageBind/requirements.txt\n!pip uninstall timm -y\n!pip install timm==0.6.7\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:26:03.535076Z","iopub.execute_input":"2025-02-18T07:26:03.535402Z","iopub.status.idle":"2025-02-18T07:26:09.185265Z","shell.execute_reply.started":"2025-02-18T07:26:03.535377Z","shell.execute_reply":"2025-02-18T07:26:09.184016Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: timm 0.6.7\nUninstalling timm-0.6.7:\n  Successfully uninstalled timm-0.6.7\nCollecting timm==0.6.7\n  Using cached timm-0.6.7-py3-none-any.whl.metadata (33 kB)\nRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.6.7) (1.13.1)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.6.7) (0.14.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (4.12.2)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (11.7.99)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->timm==0.6.7) (75.1.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->timm==0.6.7) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.7) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.7) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.7) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.7) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.7) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.7) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.7) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.7) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.7) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.7) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.7) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.7) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.7) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm==0.6.7) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm==0.6.7) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm==0.6.7) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->timm==0.6.7) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->timm==0.6.7) (2024.2.0)\nUsing cached timm-0.6.7-py3-none-any.whl (509 kB)\nInstalling collected packages: timm\nSuccessfully installed timm-0.6.7\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import einops\nimport numpy as np\nimport torch\n\nimport gzip\nimport html\nimport io\nimport math\nfrom functools import lru_cache\nfrom typing import Callable, List, Optional, Tuple\n\nimport ftfy\nimport regex as re\nimport torch\nfrom iopath.common.file_io import g_pathmgr\nfrom timm.models.layers import trunc_normal\n\nimport os\nfrom functools import partial\nfrom types import SimpleNamespace\n\n\nfrom functools import partial\nfrom typing import Callable, List, Optional\n\n\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom imagebind import data\n\nfrom imagebind.models import imagebind_model\nfrom imagebind.models.imagebind_model import ModalityType\nimport argparse\nimport random\n\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nimport torch_geometric\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom torch.utils.data import DataLoader\n\nimport torchmetrics\nfrom tqdm import tqdm\nimport torch.multiprocessing as mp\ntorch.use_deterministic_algorithms(False)\nfrom transformers import BertTokenizer, BertModel\n\nimport clip\nfrom transformers import ViTModel, ViTFeatureExtractor\nfrom sklearn.metrics import classification_report\nimport sys\nfrom datetime import datetime\n\nfrom pytorch_lightning import Trainer\n\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat\nfrom einops_exts import rearrange_many, repeat_many\n\nimport logging\nimport math\nimport pkg_resources\n\nimport torchaudio\nfrom PIL import Image\nfrom pytorchvideo import transforms as pv_transforms\nfrom pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torchvision import transforms\nfrom torchvision.transforms._transforms_video import NormalizeVideo\n\n#from imagebind.models.multimodal_preprocessors import SimpleTokenizer\n\nDEFAULT_AUDIO_FRAME_SHIFT_MS = 10  # in milliseconds\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport torch.nn.functional as F\n\nimport clip\nimport torch.nn as nn\nimport torchmetrics\nfrom clip import clip\nfrom tqdm import tqdm\nfrom functools import partial\n\ntorch.set_default_dtype(torch.float32)\nimport pandas as pd\nimport spacy\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\n#from sentence_transformers import SentenceTransformer\nimport argparse\nimport random\n#from sentence_transformers import SentenceTransformer\nfrom torch.utils.data import DataLoader\nimport torchmetrics\nimport torch.multiprocessing as mp\ntorch.use_deterministic_algorithms(False)\nimport torch\nimport clip\nfrom sklearn.metrics import classification_report\nimport sys\nfrom datetime import datetime\nfrom functools import partial\nfrom transformers import AutoTokenizer\ntorch.set_default_dtype(torch.float32)\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport pytorch_lightning as pl\n\n\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport torch\n\n#from languagebind import LanguageBind, to_device, transform_dict, LanguageBindImageTokenizer\nfrom PIL import Image\nimport torchvision.transforms as transforms\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\ntorch.set_default_dtype(torch.float32)\nclip_variant = \"ViT-L/14\"\n#from configs import cfg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:26:40.978351Z","iopub.execute_input":"2025-02-18T07:26:40.978699Z","iopub.status.idle":"2025-02-18T07:26:41.001214Z","shell.execute_reply.started":"2025-02-18T07:26:40.978666Z","shell.execute_reply":"2025-02-18T07:26:40.999980Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-871cfc48d6a3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0miopath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mg_pathmgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrunc_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'trunc_normal' from 'timm.models.layers' (/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'trunc_normal' from 'timm.models.layers' (/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py)","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# !pip install git+https://github.com/openai/CLIP.git\n# !pip install torch_geometric\n# !pip install einops_exts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.542274Z","iopub.status.idle":"2025-02-18T07:25:33.542900Z","shell.execute_reply":"2025-02-18T07:25:33.542570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install pytorch_lightning\n# !pip uninstall scipy pytorch-lightning zmq -y\n# !pip install scipy pytorch-lightning zmq\n# !pip install --upgrade scipy\n# !pip install yacs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.544010Z","iopub.status.idle":"2025-02-18T07:25:33.544418Z","shell.execute_reply":"2025-02-18T07:25:33.544247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nclass Normalize(nn.Module):\n    def __init__(self, dim: int) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n\n\nclass LearnableLogitScaling(nn.Module):\n    def __init__(\n        self,\n        logit_scale_init: float = 1 / 0.07,\n        learnable: bool = True,\n        max_logit_scale: float = 100,\n    ) -> None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.logit_scale_init = logit_scale_init\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.logit_scale_init)\n        if learnable:\n            self.log_logit_scale = nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x):\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self):\n        st = f\"logit_scale_init={self.logit_scale_init},learnable={self.learnable},\" \\\n             f\" max_logit_scale={self.max_logit_scale}\"\n        return st\n\n\nclass EinOpsRearrange(nn.Module):\n    def __init__(self, rearrange_expr: str, **kwargs) -> None:\n        super().__init__()\n        self.rearrange_expr = rearrange_expr\n        self.kwargs = kwargs\n\n    def forward(self, x):\n        assert isinstance(x, torch.Tensor)\n        return einops.rearrange(x, self.rearrange_expr, **self.kwargs)\n\n\nclass VerboseNNModule(nn.Module):\n    \"\"\"\n    Wrapper around nn.Module that prints registered buffers and parameter names.\n    \"\"\"\n\n    @staticmethod\n    def get_readable_tensor_repr(name: str, tensor: torch.Tensor) -> str:\n        st = (\n            \"(\"\n            + name\n            + \"): \"\n            + \"tensor(\"\n            + str(tuple(tensor[1].shape))\n            + \", requires_grad=\"\n            + str(tensor[1].requires_grad)\n            + \")\\n\"\n        )\n        return st\n\n    def extra_repr(self) -> str:\n        named_modules = set()\n        for p in self.named_modules():\n            named_modules.update([p[0]])\n        named_modules = list(named_modules)\n\n        string_repr = \"\"\n        for p in self.named_parameters():\n            name = p[0].split(\".\")[0]\n            if name not in named_modules:\n                string_repr += self.get_readable_tensor_repr(name, p)\n\n        for p in self.named_buffers():\n            name = p[0].split(\".\")[0]\n            string_repr += self.get_readable_tensor_repr(name, p)\n\n        return string_repr\n\n\ndef cast_if_src_dtype(\n    tensor: torch.Tensor, src_dtype: torch.dtype, tgt_dtype: torch.dtype\n):\n    updated = False\n    if tensor.dtype == src_dtype:\n        tensor = tensor.to(dtype=tgt_dtype)\n        updated = True\n    return tensor, updated\n\n\nclass QuickGELU(nn.Module):\n    # From https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L166\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass SelectElement(nn.Module):\n    def __init__(self, index) -> None:\n        super().__init__()\n        self.index = index\n\n    def forward(self, x):\n        assert x.ndim >= 3\n        return x[:, self.index, ...]\n\n\nclass SelectEOSAndProject(nn.Module):\n    \"\"\"\n    Text Pooling used in OpenCLIP\n    \"\"\"\n\n    def __init__(self, proj: nn.Module) -> None:\n        super().__init__()\n        self.proj = proj\n\n    def forward(self, x, seq_len):\n        assert x.ndim == 3\n        # x is of shape B x L x D\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), seq_len]\n        x = self.proj(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.658503Z","iopub.execute_input":"2025-02-18T07:25:33.658989Z","iopub.status.idle":"2025-02-18T07:25:33.690831Z","shell.execute_reply.started":"2025-02-18T07:25:33.658956Z","shell.execute_reply":"2025-02-18T07:25:33.689435Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-d152353b24e9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This source code is licensed under the license found in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"],"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"def get_sinusoid_encoding_table(n_position, d_hid):\n    \"\"\"Sinusoid position encoding table\"\"\"\n\n    # TODO: make it with torch instead of numpy\n    def get_position_angle_vec(position):\n        return [\n            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n    )\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n\ndef interpolate_pos_encoding_2d(target_spatial_size, pos_embed):\n    N = pos_embed.shape[1]\n    if N == target_spatial_size:\n        return pos_embed\n    dim = pos_embed.shape[-1]\n    # nn.functional.interpolate doesn't work with bfloat16 so we cast to float32\n    pos_embed, updated = cast_if_src_dtype(pos_embed, torch.bfloat16, torch.float32)\n    pos_embed = nn.functional.interpolate(\n        pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n            0, 3, 1, 2\n        ),\n        scale_factor=math.sqrt(target_spatial_size / N),\n        mode=\"bicubic\",\n    )\n    if updated:\n        pos_embed, _ = cast_if_src_dtype(pos_embed, torch.float32, torch.bfloat16)\n    pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return pos_embed\n\n\ndef interpolate_pos_encoding(\n    npatch_per_img,\n    pos_embed,\n    patches_layout,\n    input_shape=None,\n    first_patch_idx=1,\n):\n    assert first_patch_idx == 0 or first_patch_idx == 1, \"there is 1 CLS token or none\"\n    N = pos_embed.shape[1] - first_patch_idx  # since it's 1 if cls_token exists\n    if npatch_per_img == N:\n        return pos_embed\n\n    assert (\n        patches_layout[-1] == patches_layout[-2]\n    ), \"Interpolation of pos embed not supported for non-square layouts\"\n\n    class_emb = pos_embed[:, :first_patch_idx]\n    pos_embed = pos_embed[:, first_patch_idx:]\n\n    if input_shape is None or patches_layout[0] == 1:\n        # simple 2D pos embedding, no temporal component\n        pos_embed = interpolate_pos_encoding_2d(npatch_per_img, pos_embed)\n    elif patches_layout[0] > 1:\n        # pos embed has a temporal component\n        assert len(input_shape) == 4, \"temporal interpolation not supported\"\n        # we only support 2D interpolation in this case\n        num_frames = patches_layout[0]\n        num_spatial_tokens = patches_layout[1] * patches_layout[2]\n        pos_embed = pos_embed.view(1, num_frames, num_spatial_tokens, -1)\n        # interpolate embedding for zeroth frame\n        pos_embed = interpolate_pos_encoding_2d(\n            npatch_per_img, pos_embed[0, 0, ...].unsqueeze(0)\n        )\n    else:\n        raise ValueError(\"This type of interpolation isn't implemented\")\n\n    return torch.cat((class_emb, pos_embed), dim=1)\n\n\ndef _get_pos_embedding(\n    npatch_per_img,\n    pos_embed,\n    patches_layout,\n    input_shape,\n    first_patch_idx=1,\n):\n    pos_embed = interpolate_pos_encoding(\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape=input_shape,\n        first_patch_idx=first_patch_idx,\n    )\n    return pos_embed\n\n\nclass PatchEmbedGeneric(nn.Module):\n    \"\"\"\n    PatchEmbed from Hydra\n    \"\"\"\n\n    def __init__(self, proj_stem, norm_layer: Optional[nn.Module] = None):\n        super().__init__()\n\n        if len(proj_stem) > 1:\n            self.proj = nn.Sequential(*proj_stem)\n        else:\n            # Special case to be able to load pre-trained models that were\n            # trained with a standard stem\n            self.proj = proj_stem[0]\n        self.norm_layer = norm_layer\n\n    def get_patch_layout(self, img_size):\n        with torch.no_grad():\n            dummy_img = torch.zeros(\n                [\n                    1,\n                ]\n                + img_size\n            )\n            dummy_out = self.proj(dummy_img)\n        embed_dim = dummy_out.shape[1]\n        patches_layout = tuple(dummy_out.shape[2:])\n        num_patches = np.prod(patches_layout)\n        return patches_layout, num_patches, embed_dim\n\n    def forward(self, x):\n        x = self.proj(x)\n        # B C (T) H W -> B (T)HW C\n        x = x.flatten(2).transpose(1, 2)\n        if self.norm_layer is not None:\n            x = self.norm_layer(x)\n        return x\n\n\nclass SpatioTemporalPosEmbeddingHelper(VerboseNNModule):\n    def __init__(\n        self,\n        patches_layout: List,\n        num_patches: int,\n        num_cls_tokens: int,\n        embed_dim: int,\n        learnable: bool,\n    ) -> None:\n        super().__init__()\n        self.num_cls_tokens = num_cls_tokens\n        self.patches_layout = patches_layout\n        self.num_patches = num_patches\n        self.num_tokens = num_cls_tokens + num_patches\n        self.learnable = learnable\n        if self.learnable:\n            self.pos_embed = nn.Parameter(torch.zeros(1, self.num_tokens, embed_dim))\n            trunc_normal_(self.pos_embed, std=0.02)\n        else:\n            self.register_buffer(\n                \"pos_embed\", get_sinusoid_encoding_table(self.num_tokens, embed_dim)\n            )\n\n    def get_pos_embedding(self, vision_input, all_vision_tokens):\n        input_shape = vision_input.shape\n        pos_embed = _get_pos_embedding(\n            all_vision_tokens.size(1) - self.num_cls_tokens,\n            pos_embed=self.pos_embed,\n            patches_layout=self.patches_layout,\n            input_shape=input_shape,\n            first_patch_idx=self.num_cls_tokens,\n        )\n        return pos_embed\n\n\nclass RGBDTPreprocessor(VerboseNNModule):\n    def __init__(\n        self,\n        rgbt_stem: PatchEmbedGeneric,\n        depth_stem: Optional[PatchEmbedGeneric],\n        img_size: Tuple = (3, 224, 224),\n        num_cls_tokens: int = 1,\n        pos_embed_fn: Optional[Callable] = None,\n        use_type_embed: bool = False,\n        init_param_style: str = \"openclip\",\n    ) -> None:\n        super().__init__()\n        stem = rgbt_stem if rgbt_stem is not None else depth_stem\n        (\n            self.patches_layout,\n            self.num_patches,\n            self.embed_dim,\n        ) = stem.get_patch_layout(img_size)\n        self.rgbt_stem = rgbt_stem\n        self.depth_stem = depth_stem\n        self.use_pos_embed = pos_embed_fn is not None\n        self.use_type_embed = use_type_embed\n        self.num_cls_tokens = num_cls_tokens\n\n        if self.use_pos_embed:\n            self.pos_embedding_helper = pos_embed_fn(\n                patches_layout=self.patches_layout,\n                num_cls_tokens=num_cls_tokens,\n                num_patches=self.num_patches,\n                embed_dim=self.embed_dim,\n            )\n        if self.num_cls_tokens > 0:\n            self.cls_token = nn.Parameter(\n                torch.zeros(1, self.num_cls_tokens, self.embed_dim)\n            )\n        if self.use_type_embed:\n            self.type_embed = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n\n        self.init_parameters(init_param_style)\n\n    @torch.no_grad()\n    def init_parameters(self, init_param_style):\n        if init_param_style == \"openclip\":\n            # OpenCLIP style initialization\n            scale = self.embed_dim**-0.5\n            if self.use_pos_embed:\n                nn.init.normal_(self.pos_embedding_helper.pos_embed)\n                self.pos_embedding_helper.pos_embed *= scale\n\n            if self.num_cls_tokens > 0:\n                nn.init.normal_(self.cls_token)\n                self.cls_token *= scale\n        elif init_param_style == \"vit\":\n            self.cls_token.data.fill_(0)\n        else:\n            raise ValueError(f\"Unknown init {init_param_style}\")\n\n        if self.use_type_embed:\n            nn.init.normal_(self.type_embed)\n\n    def tokenize_input_and_cls_pos(self, input, stem, mask):\n        # tokens is of shape B x L x D\n        tokens = stem(input)\n        assert tokens.ndim == 3\n        assert tokens.shape[2] == self.embed_dim\n        B = tokens.shape[0]\n        if self.num_cls_tokens > 0:\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            tokens = torch.cat((class_tokens, tokens), dim=1)\n        if self.use_pos_embed:\n            pos_embed = self.pos_embedding_helper.get_pos_embedding(input, tokens)\n            tokens = tokens + pos_embed\n        if self.use_type_embed:\n            tokens = tokens + self.type_embed.expand(B, -1, -1)\n        return tokens\n\n    def forward(self, vision=None, depth=None, patch_mask=None):\n        if patch_mask is not None:\n            raise NotImplementedError()\n\n        if vision is not None:\n            vision_tokens = self.tokenize_input_and_cls_pos(\n                vision, self.rgbt_stem, patch_mask\n            )\n\n        if depth is not None:\n            depth_tokens = self.tokenize_input_and_cls_pos(\n                depth, self.depth_stem, patch_mask\n            )\n\n        # aggregate tokens\n        if vision is not None and depth is not None:\n            final_tokens = vision_tokens + depth_tokens\n        else:\n            final_tokens = vision_tokens if vision is not None else depth_tokens\n        return_dict = {\n            \"trunk\": {\n                \"tokens\": final_tokens,\n            },\n            \"head\": {},\n        }\n        return return_dict\n\n\nclass AudioPreprocessor(RGBDTPreprocessor):\n    def __init__(self, audio_stem: PatchEmbedGeneric, **kwargs) -> None:\n        super().__init__(rgbt_stem=audio_stem, depth_stem=None, **kwargs)\n\n    def forward(self, audio=None):\n        return super().forward(vision=audio)\n\n\nclass ThermalPreprocessor(RGBDTPreprocessor):\n    def __init__(self, thermal_stem: PatchEmbedGeneric, **kwargs) -> None:\n        super().__init__(rgbt_stem=thermal_stem, depth_stem=None, **kwargs)\n\n    def forward(self, thermal=None):\n        return super().forward(vision=thermal)\n\n\ndef build_causal_attention_mask(context_length):\n    # lazily create causal attention mask, with full attention between the vision tokens\n    # pytorch uses additive attention mask; fill with -inf\n    mask = torch.empty(context_length, context_length, requires_grad=False)\n    mask.fill_(float(\"-inf\"))\n    mask.triu_(1)  # zero out the lower diagonal\n    return mask\n\n\nclass TextPreprocessor(VerboseNNModule):\n    def __init__(\n        self,\n        vocab_size: int,\n        context_length: int,\n        embed_dim: int,\n        causal_masking: bool,\n        supply_seq_len_to_head: bool = True,\n        num_cls_tokens: int = 0,\n        init_param_style: str = \"openclip\",\n    ) -> None:\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.context_length = context_length\n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n        self.pos_embed = nn.Parameter(\n            torch.empty(1, self.context_length + num_cls_tokens, embed_dim)\n        )\n        self.causal_masking = causal_masking\n        if self.causal_masking:\n            mask = build_causal_attention_mask(self.context_length)\n            # register the mask as a buffer so it can be moved to the right device\n            self.register_buffer(\"mask\", mask)\n\n        self.supply_seq_len_to_head = supply_seq_len_to_head\n        self.num_cls_tokens = num_cls_tokens\n        self.embed_dim = embed_dim\n        if num_cls_tokens > 0:\n            assert self.causal_masking is False, \"Masking + CLS token isn't implemented\"\n            self.cls_token = nn.Parameter(\n                torch.zeros(1, self.num_cls_tokens, embed_dim)\n            )\n\n        self.init_parameters(init_param_style)\n\n    @torch.no_grad()\n    def init_parameters(self, init_param_style=\"openclip\"):\n        # OpenCLIP style initialization\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.pos_embed, std=0.01)\n\n        if init_param_style == \"openclip\":\n            # OpenCLIP style initialization\n            scale = self.embed_dim**-0.5\n            if self.num_cls_tokens > 0:\n                nn.init.normal_(self.cls_token)\n                self.cls_token *= scale\n        elif init_param_style == \"vit\":\n            self.cls_token.data.fill_(0)\n        else:\n            raise ValueError(f\"Unknown init {init_param_style}\")\n\n    def forward(self, text):\n        # text tokens are of shape B x L x D\n        text_tokens = self.token_embedding(text)\n        # concat CLS tokens if any\n        if self.num_cls_tokens > 0:\n            B = text_tokens.shape[0]\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            text_tokens = torch.cat((class_tokens, text_tokens), dim=1)\n        text_tokens = text_tokens + self.pos_embed\n        return_dict = {\n            \"trunk\": {\n                \"tokens\": text_tokens,\n            },\n            \"head\": {},\n        }\n        # Compute sequence length after adding CLS tokens\n        if self.supply_seq_len_to_head:\n            text_lengths = text.argmax(dim=-1)\n            return_dict[\"head\"] = {\n                \"seq_len\": text_lengths,\n            }\n        if self.causal_masking:\n            return_dict[\"trunk\"].update({\"attn_mask\": self.mask})\n        return return_dict\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass PadIm2Video(Im2Video):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__(time_dim=time_dim)\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        x = super().forward(x)\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = nn.functional.pad(x, padarg)\n        return x\n\n\n# Modified from github.com/openai/CLIP\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = (\n        list(range(ord(\"!\"), ord(\"~\") + 1))\n        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    )\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str, context_length=77):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n\n        with g_pathmgr.open(bpe_path, \"rb\") as fh:\n            bpe_bytes = io.BytesIO(fh.read())\n            merges: List[str] = gzip.open(bpe_bytes).read().decode(\"utf-8\").split(\"\\n\")\n        merges = merges[1 : 49152 - 256 - 2 + 1]\n        merges: List[Tuple[str, ...]] = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + \"</w>\" for v in vocab]\n        for merge in merges:\n            vocab.append(\"\".join(merge))\n        vocab.extend([\"<|startoftext|>\", \"<|endoftext|>\"])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {\n            \"<|startoftext|>\": \"<|startoftext|>\",\n            \"<|endoftext|>\": \"<|endoftext|>\",\n        }\n        self.pat = re.compile(\n            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n            re.IGNORECASE,\n        )\n        self.context_length = context_length\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token + \"</w>\"\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(\n                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n            )\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = (\n            bytearray([self.byte_decoder[c] for c in text])\n            .decode(\"utf-8\", errors=\"replace\")\n            .replace(\"</w>\", \" \")\n        )\n        return text\n\n    def __call__(self, texts, context_length=None):\n        if not context_length:\n            context_length = self.context_length\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        sot_token = self.encoder[\"<|startoftext|>\"]\n        eot_token = self.encoder[\"<|endoftext|>\"]\n        all_tokens = [[sot_token] + self.encode(text) + [eot_token] for text in texts]\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n        for i, tokens in enumerate(all_tokens):\n            tokens = tokens[:context_length]\n            result[i, : len(tokens)] = torch.tensor(tokens)\n\n        if len(result) == 1:\n            return result[0]\n        return result\n\n\nclass IMUPreprocessor(VerboseNNModule):\n    def __init__(\n        self,\n        kernel_size: int,\n        imu_stem: PatchEmbedGeneric,\n        embed_dim: int,\n        img_size: Tuple = (6, 2000),\n        num_cls_tokens: int = 1,\n        pos_embed_fn: Optional[Callable] = None,\n        init_param_style: str = \"openclip\",\n    ) -> None:\n        super().__init__()\n        self.imu_stem = imu_stem\n        self.embed_dim = embed_dim\n        self.use_pos_embed = pos_embed_fn is not None\n        self.num_cls_tokens = num_cls_tokens\n        self.kernel_size = kernel_size\n        self.pos_embed = nn.Parameter(\n            torch.empty(1, (img_size[1] // kernel_size) + num_cls_tokens, embed_dim)\n        )\n\n        if self.num_cls_tokens > 0:\n            self.cls_token = nn.Parameter(\n                torch.zeros(1, self.num_cls_tokens, self.embed_dim)\n            )\n\n        self.init_parameters(init_param_style)\n\n    @torch.no_grad()\n    def init_parameters(self, init_param_style):\n        nn.init.normal_(self.pos_embed, std=0.01)\n\n        if init_param_style == \"openclip\":\n            # OpenCLIP style initialization\n            scale = self.embed_dim**-0.5\n\n            if self.num_cls_tokens > 0:\n                nn.init.normal_(self.cls_token)\n                self.cls_token *= scale\n        elif init_param_style == \"vit\":\n            self.cls_token.data.fill_(0)\n        else:\n            raise ValueError(f\"Unknown init {init_param_style}\")\n\n    def tokenize_input_and_cls_pos(self, input, stem):\n        # tokens is of shape B x L x D\n        tokens = stem.norm_layer(stem.proj(input))\n        assert tokens.ndim == 3\n        assert tokens.shape[2] == self.embed_dim\n        B = tokens.shape[0]\n        if self.num_cls_tokens > 0:\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            tokens = torch.cat((class_tokens, tokens), dim=1)\n        if self.use_pos_embed:\n            tokens = tokens + self.pos_embed\n        return tokens\n\n    def forward(self, imu):\n        # Patchify\n        imu = imu.unfold(\n            -1,\n            self.kernel_size,\n            self.kernel_size,\n        ).permute(0, 2, 1, 3)\n        imu = imu.reshape(imu.size(0), imu.size(1), -1)\n\n        imu_tokens = self.tokenize_input_and_cls_pos(\n            imu,\n            self.imu_stem,\n        )\n\n        return_dict = {\n            \"trunk\": {\n                \"tokens\": imu_tokens,\n            },\n            \"head\": {},\n        }\n        return return_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.691539Z","iopub.status.idle":"2025-02-18T07:25:33.691863Z","shell.execute_reply":"2025-02-18T07:25:33.691737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nModalityType = SimpleNamespace(\n    VISION=\"vision\",\n    TEXT=\"text\",\n    AUDIO=\"audio\",\n    THERMAL=\"thermal\",\n    DEPTH=\"depth\",\n    IMU=\"imu\",\n)\n\n\nclass ImageBindModel(nn.Module):\n    def __init__(\n        self,\n        video_frames=2,\n        kernel_size=(2, 14, 14),\n        audio_kernel_size=16,\n        audio_stride=10,\n        out_embed_dim=768,\n        vision_embed_dim=1024,\n        vision_num_blocks=24,\n        vision_num_heads=16,\n        audio_embed_dim=768,\n        audio_num_blocks=12,\n        audio_num_heads=12,\n        audio_num_mel_bins=128,\n        audio_target_len=204,\n        audio_drop_path=0.1,\n        text_embed_dim=768,\n        text_num_blocks=12,\n        text_num_heads=12,\n        depth_embed_dim=384,\n        depth_kernel_size=16,\n        depth_num_blocks=12,\n        depth_num_heads=8,\n        depth_drop_path=0.0,\n        thermal_embed_dim=768,\n        thermal_kernel_size=16,\n        thermal_num_blocks=12,\n        thermal_num_heads=12,\n        thermal_drop_path=0.0,\n        imu_embed_dim=512,\n        imu_kernel_size=8,\n        imu_num_blocks=6,\n        imu_num_heads=8,\n        imu_drop_path=0.7,\n    ):\n        super().__init__()\n\n        self.modality_preprocessors = self._create_modality_preprocessors(\n            video_frames,\n            vision_embed_dim,\n            kernel_size,\n            text_embed_dim,\n            audio_embed_dim,\n            audio_kernel_size,\n            audio_stride,\n            audio_num_mel_bins,\n            audio_target_len,\n            depth_embed_dim,\n            depth_kernel_size,\n            thermal_embed_dim,\n            thermal_kernel_size,\n            imu_embed_dim,\n        )\n\n        self.modality_trunks = self._create_modality_trunks(\n            vision_embed_dim,\n            vision_num_blocks,\n            vision_num_heads,\n            text_embed_dim,\n            text_num_blocks,\n            text_num_heads,\n            audio_embed_dim,\n            audio_num_blocks,\n            audio_num_heads,\n            audio_drop_path,\n            depth_embed_dim,\n            depth_num_blocks,\n            depth_num_heads,\n            depth_drop_path,\n            thermal_embed_dim,\n            thermal_num_blocks,\n            thermal_num_heads,\n            thermal_drop_path,\n            imu_embed_dim,\n            imu_num_blocks,\n            imu_num_heads,\n            imu_drop_path,\n        )\n\n        self.modality_heads = self._create_modality_heads(\n            out_embed_dim,\n            vision_embed_dim,\n            text_embed_dim,\n            audio_embed_dim,\n            depth_embed_dim,\n            thermal_embed_dim,\n            imu_embed_dim,\n        )\n\n        self.modality_postprocessors = self._create_modality_postprocessors(\n            out_embed_dim\n        )\n\n    def _create_modality_preprocessors(\n        self,\n        video_frames=2,\n        vision_embed_dim=1024,\n        kernel_size=(2, 14, 14),\n        text_embed_dim=768,\n        audio_embed_dim=768,\n        audio_kernel_size=16,\n        audio_stride=10,\n        audio_num_mel_bins=128,\n        audio_target_len=204,\n        depth_embed_dim=768,\n        depth_kernel_size=16,\n        thermal_embed_dim=768,\n        thermal_kernel_size=16,\n        imu_embed_dim=512,\n    ):\n        rgbt_stem = PatchEmbedGeneric(\n            proj_stem=[\n                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n                nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=kernel_size,\n                    out_channels=vision_embed_dim,\n                    stride=kernel_size,\n                    bias=False,\n                ),\n            ]\n        )\n        rgbt_preprocessor = RGBDTPreprocessor(\n            img_size=[3, video_frames, 224, 224],\n            num_cls_tokens=1,\n            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n            rgbt_stem=rgbt_stem,\n            depth_stem=None,\n        )\n\n        text_preprocessor = TextPreprocessor(\n            context_length=77,\n            vocab_size=49408,\n            embed_dim=text_embed_dim,\n            causal_masking=True,\n        )\n\n        audio_stem = PatchEmbedGeneric(\n            proj_stem=[\n                nn.Conv2d(\n                    in_channels=1,\n                    kernel_size=audio_kernel_size,\n                    stride=audio_stride,\n                    out_channels=audio_embed_dim,\n                    bias=False,\n                ),\n            ],\n            norm_layer=nn.LayerNorm(normalized_shape=audio_embed_dim),\n        )\n        audio_preprocessor = AudioPreprocessor(\n            img_size=[1, audio_num_mel_bins, audio_target_len],\n            num_cls_tokens=1,\n            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n            audio_stem=audio_stem,\n        )\n\n        depth_stem = PatchEmbedGeneric(\n            [\n                nn.Conv2d(\n                    kernel_size=depth_kernel_size,\n                    in_channels=1,\n                    out_channels=depth_embed_dim,\n                    stride=depth_kernel_size,\n                    bias=False,\n                ),\n            ],\n            norm_layer=nn.LayerNorm(normalized_shape=depth_embed_dim),\n        )\n\n        depth_preprocessor = RGBDTPreprocessor(\n            img_size=[1, 224, 224],\n            num_cls_tokens=1,\n            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n            rgbt_stem=None,\n            depth_stem=depth_stem,\n        )\n\n        thermal_stem = PatchEmbedGeneric(\n            [\n                nn.Conv2d(\n                    kernel_size=thermal_kernel_size,\n                    in_channels=1,\n                    out_channels=thermal_embed_dim,\n                    stride=thermal_kernel_size,\n                    bias=False,\n                ),\n            ],\n            norm_layer=nn.LayerNorm(normalized_shape=thermal_embed_dim),\n        )\n        thermal_preprocessor = ThermalPreprocessor(\n            img_size=[1, 224, 224],\n            num_cls_tokens=1,\n            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n            thermal_stem=thermal_stem,\n        )\n\n        imu_stem = PatchEmbedGeneric(\n            [\n                nn.Linear(\n                    in_features=48,\n                    out_features=imu_embed_dim,\n                    bias=False,\n                ),\n            ],\n            norm_layer=nn.LayerNorm(normalized_shape=imu_embed_dim),\n        )\n\n        imu_preprocessor = IMUPreprocessor(\n            img_size=[6, 2000],\n            num_cls_tokens=1,\n            kernel_size=8,\n            embed_dim=imu_embed_dim,\n            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n            imu_stem=imu_stem,\n        )\n\n        modality_preprocessors = {\n            ModalityType.VISION: rgbt_preprocessor,\n            ModalityType.TEXT: text_preprocessor,\n            ModalityType.AUDIO: audio_preprocessor,\n            ModalityType.DEPTH: depth_preprocessor,\n            ModalityType.THERMAL: thermal_preprocessor,\n            ModalityType.IMU: imu_preprocessor,\n        }\n\n        return nn.ModuleDict(modality_preprocessors)\n\n    def _create_modality_trunks(\n        self,\n        vision_embed_dim=1024,\n        vision_num_blocks=24,\n        vision_num_heads=16,\n        text_embed_dim=768,\n        text_num_blocks=12,\n        text_num_heads=12,\n        audio_embed_dim=768,\n        audio_num_blocks=12,\n        audio_num_heads=12,\n        audio_drop_path=0.0,\n        depth_embed_dim=768,\n        depth_num_blocks=12,\n        depth_num_heads=12,\n        depth_drop_path=0.0,\n        thermal_embed_dim=768,\n        thermal_num_blocks=12,\n        thermal_num_heads=12,\n        thermal_drop_path=0.0,\n        imu_embed_dim=512,\n        imu_num_blocks=6,\n        imu_num_heads=8,\n        imu_drop_path=0.7,\n    ):\n        def instantiate_trunk(\n            embed_dim, num_blocks, num_heads, pre_transformer_ln, add_bias_kv, drop_path\n        ):\n            return SimpleTransformer(\n                embed_dim=embed_dim,\n                num_blocks=num_blocks,\n                ffn_dropout_rate=0.0,\n                drop_path_rate=drop_path,\n                attn_target=partial(\n                    MultiheadAttention,\n                    embed_dim=embed_dim,\n                    num_heads=num_heads,\n                    bias=True,\n                    add_bias_kv=add_bias_kv,\n                ),\n                pre_transformer_layer=nn.Sequential(\n                    nn.LayerNorm(embed_dim, eps=1e-6)\n                    if pre_transformer_ln\n                    else nn.Identity(),\n                    EinOpsRearrange(\"b l d -> l b d\"),\n                ),\n                post_transformer_layer=EinOpsRearrange(\"l b d -> b l d\"),\n            )\n\n        modality_trunks = {}\n        modality_trunks[ModalityType.VISION] = instantiate_trunk(\n            vision_embed_dim,\n            vision_num_blocks,\n            vision_num_heads,\n            pre_transformer_ln=True,\n            add_bias_kv=False,\n            drop_path=0.0,\n        )\n        modality_trunks[ModalityType.TEXT] = instantiate_trunk(\n            text_embed_dim,\n            text_num_blocks,\n            text_num_heads,\n            pre_transformer_ln=False,\n            add_bias_kv=False,\n            drop_path=0.0,\n        )\n        modality_trunks[ModalityType.AUDIO] = instantiate_trunk(\n            audio_embed_dim,\n            audio_num_blocks,\n            audio_num_heads,\n            pre_transformer_ln=False,\n            add_bias_kv=True,\n            drop_path=audio_drop_path,\n        )\n        modality_trunks[ModalityType.DEPTH] = instantiate_trunk(\n            depth_embed_dim,\n            depth_num_blocks,\n            depth_num_heads,\n            pre_transformer_ln=False,\n            add_bias_kv=True,\n            drop_path=depth_drop_path,\n        )\n        modality_trunks[ModalityType.THERMAL] = instantiate_trunk(\n            thermal_embed_dim,\n            thermal_num_blocks,\n            thermal_num_heads,\n            pre_transformer_ln=False,\n            add_bias_kv=True,\n            drop_path=thermal_drop_path,\n        )\n        modality_trunks[ModalityType.IMU] = instantiate_trunk(\n            imu_embed_dim,\n            imu_num_blocks,\n            imu_num_heads,\n            pre_transformer_ln=False,\n            add_bias_kv=True,\n            drop_path=imu_drop_path,\n        )\n\n        return nn.ModuleDict(modality_trunks)\n\n    def _create_modality_heads(\n        self,\n        out_embed_dim,\n        vision_embed_dim,\n        text_embed_dim,\n        audio_embed_dim,\n        depth_embed_dim,\n        thermal_embed_dim,\n        imu_embed_dim,\n    ):\n        modality_heads = {}\n\n        modality_heads[ModalityType.VISION] = nn.Sequential(\n            nn.LayerNorm(normalized_shape=vision_embed_dim, eps=1e-6),\n            SelectElement(index=0),\n            nn.Linear(vision_embed_dim, out_embed_dim, bias=False),\n        )\n\n        modality_heads[ModalityType.TEXT] = SelectEOSAndProject(\n            proj=nn.Sequential(\n                nn.LayerNorm(normalized_shape=text_embed_dim, eps=1e-6),\n                nn.Linear(text_embed_dim, out_embed_dim, bias=False),\n            )\n        )\n\n        modality_heads[ModalityType.AUDIO] = nn.Sequential(\n            nn.LayerNorm(normalized_shape=audio_embed_dim, eps=1e-6),\n            SelectElement(index=0),\n            nn.Linear(audio_embed_dim, out_embed_dim, bias=False),\n        )\n\n        modality_heads[ModalityType.DEPTH] = nn.Sequential(\n            nn.LayerNorm(normalized_shape=depth_embed_dim, eps=1e-6),\n            SelectElement(index=0),\n            nn.Linear(depth_embed_dim, out_embed_dim, bias=False),\n        )\n\n        modality_heads[ModalityType.THERMAL] = nn.Sequential(\n            nn.LayerNorm(normalized_shape=thermal_embed_dim, eps=1e-6),\n            SelectElement(index=0),\n            nn.Linear(thermal_embed_dim, out_embed_dim, bias=False),\n        )\n\n        modality_heads[ModalityType.IMU] = nn.Sequential(\n            nn.LayerNorm(normalized_shape=imu_embed_dim, eps=1e-6),\n            SelectElement(index=0),\n            nn.Dropout(p=0.5),\n            nn.Linear(imu_embed_dim, out_embed_dim, bias=False),\n        )\n\n        return nn.ModuleDict(modality_heads)\n\n    def _create_modality_postprocessors(self, out_embed_dim):\n        modality_postprocessors = {}\n\n        modality_postprocessors[ModalityType.VISION] = Normalize(dim=-1)\n        modality_postprocessors[ModalityType.TEXT] = nn.Sequential(\n            Normalize(dim=-1), LearnableLogitScaling(learnable=True)\n        )\n        modality_postprocessors[ModalityType.AUDIO] = nn.Sequential(\n            Normalize(dim=-1),\n            LearnableLogitScaling(logit_scale_init=20.0, learnable=False),\n        )\n        modality_postprocessors[ModalityType.DEPTH] = nn.Sequential(\n            Normalize(dim=-1),\n            LearnableLogitScaling(logit_scale_init=5.0, learnable=False),\n        )\n        modality_postprocessors[ModalityType.THERMAL] = nn.Sequential(\n            Normalize(dim=-1),\n            LearnableLogitScaling(logit_scale_init=10.0, learnable=False),\n        )\n        modality_postprocessors[ModalityType.IMU] = nn.Sequential(\n            Normalize(dim=-1),\n            LearnableLogitScaling(logit_scale_init=5.0, learnable=False),\n        )\n\n        return nn.ModuleDict(modality_postprocessors)\n\n    def forward(self, inputs):\n        outputs = {}\n        for modality_key, modality_value in inputs.items():\n            reduce_list = (\n                modality_value.ndim >= 5\n            )  # Audio and Video inputs consist of multiple clips\n            if reduce_list:\n                B, S = modality_value.shape[:2]\n                modality_value = modality_value.reshape(\n                    B * S, *modality_value.shape[2:]\n                )\n\n            if modality_value is not None:\n                modality_value = self.modality_preprocessors[modality_key](\n                    **{modality_key: modality_value}\n                )\n                trunk_inputs = modality_value[\"trunk\"]\n                head_inputs = modality_value[\"head\"]\n                modality_value = self.modality_trunks[modality_key](**trunk_inputs)\n                modality_value = self.modality_heads[modality_key](\n                    modality_value, **head_inputs\n                )\n                modality_value = self.modality_postprocessors[modality_key](\n                    modality_value\n                )\n\n                if reduce_list:\n                    modality_value = modality_value.reshape(B, S, -1)\n                    modality_value = modality_value.mean(dim=1)\n\n                outputs[modality_key] = modality_value\n\n        return outputs\n\n\ndef imagebind_huge(pretrained=False):\n    model = ImageBindModel(\n        vision_embed_dim=1280,\n        vision_num_blocks=32,\n        vision_num_heads=16,\n        text_embed_dim=1024,\n        text_num_blocks=24,\n        text_num_heads=16,\n        out_embed_dim=1024,\n        audio_drop_path=0.1,\n        imu_drop_path=0.7,\n    )\n\n    if pretrained:\n        if not os.path.exists(\".checkpoints/imagebind_huge.pth\"):\n            print(\n                \"Downloading imagebind weights to .checkpoints/imagebind_huge.pth ...\"\n            )\n            os.makedirs(\".checkpoints\", exist_ok=True)\n            torch.hub.download_url_to_file(\n                \"https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth\",\n                \".checkpoints/imagebind_huge.pth\",\n                progress=True,\n            )\n\n        model.load_state_dict(torch.load(\".checkpoints/imagebind_huge.pth\"))\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.842198Z","iopub.execute_input":"2025-02-18T07:25:33.842585Z","iopub.status.idle":"2025-02-18T07:25:33.888907Z","shell.execute_reply.started":"2025-02-18T07:25:33.842551Z","shell.execute_reply":"2025-02-18T07:25:33.886937Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3fd6eb0b8837>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ModalityType = SimpleNamespace(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mVISION\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vision\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTEXT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mAUDIO\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mTHERMAL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"thermal\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'SimpleNamespace' is not defined"],"ename":"NameError","evalue":"name 'SimpleNamespace' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version,\n        # can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass MultiheadAttention(nn.MultiheadAttention):\n    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor):\n        return super().forward(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n\n\nclass ViTAttention(Attention):\n    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor):\n        assert attn_mask is None\n        return super().forward(x)\n\n\nclass BlockWithMasking(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        attn_target: Callable,\n        mlp_ratio: int = 4,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = nn.LayerNorm,\n        ffn_dropout_rate: float = 0.0,\n        drop_path: float = 0.0,\n        layer_scale_type: Optional[str] = None,\n        layer_scale_init_value: float = 1e-4,\n    ):\n        super().__init__()\n\n        assert not isinstance(\n            attn_target, nn.Module\n        ), \"attn_target should be a Callable. Otherwise attn_target is shared across blocks!\"\n        self.attn = attn_target()\n        if drop_path > 0.0:\n            self.drop_path = DropPath(drop_path)\n        else:\n            self.drop_path = nn.Identity()\n        self.norm_1 = norm_layer(dim)\n        mlp_hidden_dim = int(mlp_ratio * dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=ffn_dropout_rate,\n        )\n        self.norm_2 = norm_layer(dim)\n        self.layer_scale_type = layer_scale_type\n        if self.layer_scale_type is not None:\n            assert self.layer_scale_type in [\n                \"per_channel\",\n                \"scalar\",\n            ], f\"Found Layer scale type {self.layer_scale_type}\"\n            if self.layer_scale_type == \"per_channel\":\n                # one gamma value per channel\n                gamma_shape = [1, 1, dim]\n            elif self.layer_scale_type == \"scalar\":\n                # single gamma value for all channels\n                gamma_shape = [1, 1, 1]\n            # two gammas: for each part of the fwd in the encoder\n            self.layer_scale_gamma1 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n            self.layer_scale_gamma2 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n\n    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor):\n        if self.layer_scale_type is None:\n            x = x + self.drop_path(self.attn(self.norm_1(x), attn_mask))\n            x = x + self.drop_path(self.mlp(self.norm_2(x)))\n        else:\n            x = (\n                x\n                + self.drop_path(self.attn(self.norm_1(x), attn_mask))\n                * self.layer_scale_gamma1\n            )\n            x = x + self.drop_path(self.mlp(self.norm_2(x))) * self.layer_scale_gamma2\n        return x\n\n\n_LAYER_NORM = partial(nn.LayerNorm, eps=1e-6)\n\n\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self,\n        attn_target: Callable,\n        embed_dim: int,\n        num_blocks: int,\n        block: Callable = BlockWithMasking,\n        pre_transformer_layer: Optional[Callable] = None,\n        post_transformer_layer: Optional[Callable] = None,\n        drop_path_rate: float = 0.0,\n        drop_path_type: str = \"progressive\",\n        norm_layer: Callable = _LAYER_NORM,\n        mlp_ratio: int = 4,\n        ffn_dropout_rate: float = 0.0,\n        layer_scale_type: Optional[str] = None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value: float = 1e-4,  # from cait; float\n        weight_init_style: str = \"jax\",  # possible values jax or pytorch\n    ):\n        \"\"\"\n        Simple Transformer with the following features\n        1. Supports masked attention\n        2. Supports DropPath\n        3. Supports LayerScale\n        4. Supports Dropout in Attention and FFN\n        5. Makes few assumptions about the input except that it is a Tensor\n        \"\"\"\n        super().__init__()\n        self.pre_transformer_layer = pre_transformer_layer\n        if drop_path_type == \"progressive\":\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, num_blocks)]\n        elif drop_path_type == \"uniform\":\n            dpr = [drop_path_rate for i in range(num_blocks)]\n        else:\n            raise ValueError(f\"Unknown drop_path_type: {drop_path_type}\")\n\n        self.blocks = nn.Sequential(\n            *[\n                block(\n                    dim=embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    ffn_dropout_rate=ffn_dropout_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(num_blocks)\n            ]\n        )\n        self.post_transformer_layer = post_transformer_layer\n        self.weight_init_style = weight_init_style\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            if self.weight_init_style == \"jax\":\n                # Based on MAE and official Jax ViT implementation\n                torch.nn.init.xavier_uniform_(m.weight)\n            elif self.weight_init_style == \"pytorch\":\n                # PyTorch ViT uses trunc_normal_\n                trunc_normal_(m.weight, std=0.02)\n\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward(\n        self,\n        tokens: torch.Tensor,\n        attn_mask: torch.Tensor = None,\n        use_checkpoint: bool = False,\n        checkpoint_every_n: int = 1,\n        checkpoint_blk_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"\n        Inputs\n        - tokens: data of shape N x L x D (or L x N x D depending on the attention implementation)\n        - attn: mask of shape L x L\n\n        Output\n        - x: data of shape N x L x D (or L x N x D depending on the attention implementation)\n        \"\"\"\n        if self.pre_transformer_layer:\n            tokens = self.pre_transformer_layer(tokens)\n        if use_checkpoint and checkpoint_blk_ids is None:\n            checkpoint_blk_ids = [\n                blk_id\n                for blk_id in range(len(self.blocks))\n                if blk_id % checkpoint_every_n == 0\n            ]\n        if checkpoint_blk_ids:\n            checkpoint_blk_ids = set(checkpoint_blk_ids)\n        for blk_id, blk in enumerate(self.blocks):\n            if use_checkpoint and blk_id in checkpoint_blk_ids:\n                tokens = checkpoint.checkpoint(\n                    blk, tokens, attn_mask, use_reentrant=False\n                )\n            else:\n                tokens = blk(tokens, attn_mask=attn_mask)\n        if self.post_transformer_layer:\n            tokens = self.post_transformer_layer(tokens)\n        return tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.889774Z","iopub.status.idle":"2025-02-18T07:25:33.890120Z","shell.execute_reply":"2025-02-18T07:25:33.889988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef return_bpe_path():\n    return pkg_resources.resource_filename(\n        \"imagebind\", \"bpe/bpe_simple_vocab_16e6.txt.gz\"\n    )\n\n\ndef waveform2melspec(waveform, sample_rate, num_mel_bins, target_length):\n    # Based on https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/dataloader.py#L102\n    waveform -= waveform.mean()\n    fbank = torchaudio.compliance.kaldi.fbank(\n        waveform,\n        htk_compat=True,\n        sample_frequency=sample_rate,\n        use_energy=False,\n        window_type=\"hanning\",\n        num_mel_bins=num_mel_bins,\n        dither=0.0,\n        frame_length=25,\n        frame_shift=DEFAULT_AUDIO_FRAME_SHIFT_MS,\n    )\n    # Convert to [mel_bins, num_frames] shape\n    fbank = fbank.transpose(0, 1)\n    # Pad to target_length\n    n_frames = fbank.size(1)\n    p = target_length - n_frames\n    # if p is too large (say >20%), flash a warning\n    if abs(p) / n_frames > 0.2:\n        logging.warning(\n            \"Large gap between audio n_frames(%d) and \"\n            \"target_length (%d). Is the audio_target_length \"\n            \"setting correct?\",\n            n_frames,\n            target_length,\n        )\n    # cut and pad\n    if p > 0:\n        fbank = torch.nn.functional.pad(fbank, (0, p), mode=\"constant\", value=0)\n    elif p < 0:\n        fbank = fbank[:, 0:target_length]\n    # Convert to [1, mel_bins, num_frames] shape, essentially like a 1\n    # channel image\n    fbank = fbank.unsqueeze(0)\n    return fbank\n\n\ndef get_clip_timepoints(clip_sampler, duration):\n    # Read out all clips in this video\n    all_clips_timepoints = []\n    is_last_clip = False\n    end = 0.0\n    while not is_last_clip:\n        start, end, _, _, is_last_clip = clip_sampler(end, duration, annotation=None)\n        all_clips_timepoints.append((start, end))\n    return all_clips_timepoints\n\n\ndef load_and_transform_vision_data(image_paths, device):\n    if image_paths is None:\n        return None\n\n    image_outputs = []\n\n    data_transform = transforms.Compose(\n        [\n            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711),\n            ),\n        ]\n    )\n\n    for image_path in image_paths:\n        with open(image_path, \"rb\") as fopen:\n            image = Image.open(fopen).convert(\"RGB\")\n\n        image = data_transform(image).to(device)\n        image_outputs.append(image)\n    return torch.stack(image_outputs, dim=0)\n\n\ndef load_and_transform_text(text, device):\n    if text is None:\n        return None\n    tokenizer = SimpleTokenizer(bpe_path=return_bpe_path())\n    tokens = [tokenizer(t).unsqueeze(0).to(device) for t in text]\n    tokens = torch.cat(tokens, dim=0)\n    return tokens\n\n\ndef load_and_transform_audio_data(\n    audio_paths,\n    device,\n    num_mel_bins=128,\n    target_length=204,\n    sample_rate=16000,\n    clip_duration=2,\n    clips_per_video=3,\n    mean=-4.268,\n    std=9.138,\n):\n    if audio_paths is None:\n        return None\n\n    audio_outputs = []\n    clip_sampler = ConstantClipsPerVideoSampler(\n        clip_duration=clip_duration, clips_per_video=clips_per_video\n    )\n\n    for audio_path in audio_paths:\n        waveform, sr = torchaudio.load(audio_path)\n        if sample_rate != sr:\n            waveform = torchaudio.functional.resample(\n                waveform, orig_freq=sr, new_freq=sample_rate\n            )\n        all_clips_timepoints = get_clip_timepoints(\n            clip_sampler, waveform.size(1) / sample_rate\n        )\n        all_clips = []\n        for clip_timepoints in all_clips_timepoints:\n            waveform_clip = waveform[\n                :,\n                int(clip_timepoints[0] * sample_rate) : int(\n                    clip_timepoints[1] * sample_rate\n                ),\n            ]\n            waveform_melspec = waveform2melspec(\n                waveform_clip, sample_rate, num_mel_bins, target_length\n            )\n            all_clips.append(waveform_melspec)\n\n        normalize = transforms.Normalize(mean=mean, std=std)\n        all_clips = [normalize(ac).to(device) for ac in all_clips]\n\n        all_clips = torch.stack(all_clips, dim=0)\n        audio_outputs.append(all_clips)\n\n    return torch.stack(audio_outputs, dim=0)\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Perform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to perform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images,\n            size=(height, width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\"Nothing else supported yet\")\n\n    def forward(self, videos):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n            if not self.flipped_crops_to_ext:\n                continue\n            flipped_video = transforms.functional.hflip(video)\n            for spatial_idx in self.flipped_crops_to_ext:\n                res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res\n\n\ndef load_and_transform_video_data(\n    video_paths,\n    device,\n    clip_duration=2,\n    clips_per_video=5,\n    sample_rate=16000,\n):\n    if video_paths is None:\n        return None\n\n    video_outputs = []\n    video_transform = transforms.Compose(\n        [\n            pv_transforms.ShortSideScale(224),\n            NormalizeVideo(\n                mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711),\n            ),\n        ]\n    )\n\n    clip_sampler = ConstantClipsPerVideoSampler(\n        clip_duration=clip_duration, clips_per_video=clips_per_video\n    )\n    frame_sampler = pv_transforms.UniformTemporalSubsample(num_samples=clip_duration)\n\n    for video_path in video_paths:\n        video = EncodedVideo.from_path(\n            video_path,\n            decoder=\"decord\",\n            decode_audio=False,\n            **{\"sample_rate\": sample_rate},\n        )\n\n        all_clips_timepoints = get_clip_timepoints(clip_sampler, video.duration)\n\n        all_video = []\n        for clip_timepoints in all_clips_timepoints:\n            # Read the clip, get frames\n            clip = video.get_clip(clip_timepoints[0], clip_timepoints[1])\n            if clip is None:\n                raise ValueError(\"No clip found\")\n            video_clip = frame_sampler(clip[\"video\"])\n            video_clip = video_clip / 255.0  # since this is float, need 0-1\n\n            all_video.append(video_clip)\n\n        all_video = [video_transform(clip) for clip in all_video]\n        all_video = SpatialCrop(224, num_crops=3)(all_video)\n\n        all_video = torch.stack(all_video, dim=0)\n        video_outputs.append(all_video)\n\n    return torch.stack(video_outputs, dim=0).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.891192Z","iopub.status.idle":"2025-02-18T07:25:33.891559Z","shell.execute_reply":"2025-02-18T07:25:33.891386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#from configs import cfg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.892895Z","iopub.status.idle":"2025-02-18T07:25:33.893398Z","shell.execute_reply":"2025-02-18T07:25:33.893237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install --upgrade numpy scikit-learn\n# !pip install --upgrade \"numpy<2\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.894317Z","iopub.status.idle":"2025-02-18T07:25:33.894816Z","shell.execute_reply":"2025-02-18T07:25:33.894642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass Custom_Dataset(Dataset):\n    def __init__(self, root_folder, dataset, label, split='train', image_size=224, fast=True):\n        super(Custom_Dataset, self).__init__()\n        \n        self.root_folder = root_folder\n        self.dataset = dataset\n        self.split = split\n        self.label = label\n\n        self.image_size = image_size\n        self.fast = fast\n        info_file = ''\n        self.info_file =info_file\n        self.df = pd.read_csv('/kaggle/input/pridemm2025/PrideMM/PrideMM.csv')\n        stance_mapping = {0: 'Neutral', 1: 'Support', 2: 'Oppose'}\n\n        # Create a new column 'stance_labels' based on the 'stance' column\n        self.df['stance_labels'] = self.df['stance'].map(stance_mapping)\n        self.df = self.df[self.df['split'] == self.split].reset_index(drop=True)\n        print(self.df.head())\n\n        if self.label == 'target':\n            self.df = self.df[self.df['hate'] == 1].reset_index(drop=True)\n\n        float_cols = self.df.select_dtypes(float).columns\n        self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        if row['text'] == 'None':\n            txt = 'null'\n        else:\n            txt = row['text']\n\n        image_fn = row['name']\n        img_folder='/kaggle/input/pridemm2025/PrideMM/Images'\n        image = Image.open(f\"{img_folder}/{image_fn}\").convert('RGB')\\\n            .resize((self.image_size, self.image_size))\n        text = txt\n\n        item = {\n            'image': image,\n            'text': text,\n            'label': row[self.label],\n            'idx_meme': row['name'],\n            'origin_text': txt,\n            'stance_labels':row['stance_labels']\n        }\n\n        return item\n\nclass Custom_Collator(object):\n    def __init__(self,prompt_size=512,stance_labels=[\"Neutral\", \"Support\", \"Oppose\"]):\n        \n        self.clip_model, _ = clip.load(clip_variant, device=d, jit=False)\n        _, self.clip_preprocess = clip.load(clip_variant, device=d, jit=False)\n        # clip_type = {\n        #     'image': 'LanguageBind_Image',\n        # }\n        clip_type = {\n        'video': 'LanguageBind_Video_FT',  # also LanguageBind_Video\n        'audio': 'LanguageBind_Audio_FT',  # also LanguageBind_Audio\n        'thermal': 'LanguageBind_Thermal',\n        'image': 'LanguageBind_Image',\n        'depth': 'LanguageBind_Depth',\n        }\n        self.languagebind_model = LanguageBind(clip_type=clip_type, cache_dir='./cache_dir').to(d)\n        self.languagebind_model.eval()\n\n        # Load LanguageBind tokenizer & transforms\n        pretrained_ckpt = f'lb203/LanguageBind_Image'\n        self.tokenizer = LanguageBindImageTokenizer.from_pretrained(pretrained_ckpt, cache_dir='./cache_dir/tokenizer_cache_dir')\n        self.image_transform = transform_dict['image'](self.languagebind_model.modality_config['image'])\n        self.modality_transform = {c: transform_dict[c](self.languagebind_model.modality_config[c]) for c in clip_type.keys()}\n\n        self.learnable_prompt = nn.Parameter(torch.randn(prompt_size).to(d))\n        # Define the projection layer in your class\n        self.text_projection1 = nn.Linear(2048, 1024).to(d)  # Reduce dimensionality\n        self.text_projection2 = nn.Linear(1024, 768).to(d) \n\n\n        # Stance labels to determine stance towards the target\n        self.stance_labels = stance_labels\n        self.clip_model.float().eval()\n\n    # def safe_modality_transform(self, image_input):\n    #         \"\"\"\n    #         Applies the modality transform for 'image' safely.\n    #         If image_input is a file path (str), calls modality_transform directly.\n    #         If image_input is a PIL Image, converts it to RGB, saves it temporarily,\n    #         applies the transform, and then deletes the temporary file.\n    #         \"\"\"\n    #         if isinstance(image_input, str):\n    #             # Directly call the modality transform.\n    #             return self.modality_transform['image'](image_input)\n    #         elif isinstance(image_input, Image.Image):\n    #             # Ensure image is in RGB.\n    #             image_rgb = image_input.convert(\"RGB\")\n    #             # Create a temporary file.\n    #             with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as tmp:\n    #                 temp_filename = tmp.name\n    #                 image_rgb.save(temp_filename, format=\"JPEG\")\n    #             try:\n    #                 transformed = self.modality_transform['image'](temp_filename)\n    #             finally:\n    #                 os.remove(temp_filename)\n    #             return transformed\n    #         else:\n    #             raise TypeError(\"Expected image file path (str) or a PIL Image\")\n\n    def compute_LanguageBind_features(self, text):\n            \"\"\"Computes embeddings using LanguageBind\"\"\"\n            # Use safe_modality_transform to ensure the image is properly handled.\n            #transformed_image = to_device(self.safe_modality_transform(image), d)\n            inputs = {\n                #'image': transformed_image,\n                'language': to_device(\n                    self.tokenizer(text, max_length=77, padding='max_length',\n                                   truncation=True, return_tensors='pt'),\n                    d\n                )\n            }\n            with torch.no_grad():\n                embeddings = self.languagebind_model(inputs)\n            return embeddings['language']\n    def __call__(self, batch):\n        labels = torch.LongTensor([item['label'] for item in batch])\n        idx_memes = [item['idx_meme'] for item in batch]\n\n        batch_new = {'labels': labels,\n                     'idx_memes': idx_memes,\n                     }\n        \n        image_embed_list = []\n        text_embed_list = []\n        lb_image_embed_list = []\n        lb_text_embed_list = []\n        for item in batch:\n\n            pixel_values = self.clip_preprocess(item['image']).unsqueeze(0)\n            \n            text = clip.tokenize(item['text'], context_length=77, truncate=True)\n            languagebind_text = [item['text']]\n\n\n            # image_filenames = item['idx_meme']  # Assume it's already a list\n            # img_folder = 'dataset/PrideMM/Images/'\n            # Construct full image paths\n            # image_paths = os.path.join(img_folder, image_filenames) \n            # languagebind_image = image_paths\n            # print(\"languagebind_text:\",languagebind_text)\n            # print(\"languagebind_image:\",languagebind_image)\n            #stance = item['stance_labels']             \n            masked_text = self.create_masked_text(\"[MASK]\")\n            static_text=\"The LGBTQ movement is a social movement that advocates for equal rights for LGBTQ people. It includes a variety of political, cultural, and community-based activities. \"\n            # Concatenate the learnable prompt with the masked stance text\n            #prompt = self.learnable_prompt.unsqueeze(0).expand(text.size(0), -1)  # Repeat prompt to match batch size\n            #text_with_prompt = torch.cat([prompt, masked_text], dim=1)\n            #actual fusion        \n            image_features, text_features = self.compute_CLIP_features_without_proj(self.clip_model,\n                                                                    pixel_values.to(d),\n                                                                    text.to(d))\n            lb_text_features = self.compute_LanguageBind_features(languagebind_text)\n            \n\n\n            \n            # Get CLIP features for the masked text\n            masked_text_features = self.compute_CLIP_features_without_proj2(\n                self.clip_model, masked_text.to(d))\n            source_text_features = self.compute_CLIP_features_without_proj3(\n                self.clip_model, static_text.to(d))\n            \n            # Concatenate learnable prompt at the feature level\n            prompt = self.learnable_prompt.unsqueeze(0).expand(text_features.size(0), -1)\n            \n            # Final text representation\n            # print(\"text_features\",text_features.shape)\n            # print(\"masked_text_features\",masked_text_features.shape)\n            # print(\"prompt\",prompt.shape)\n            text_features = torch.cat([text_features, masked_text_features, prompt,source_text_features], dim=1)\n            \n           # lb_image_embed_list.append(lb_image_features.cpu().detach())\n            lb_text_embed_list.append(lb_text_features.cpu().detach())\n\n            text_embed_list.append(text_features.cpu().detach())\n            image_embed_list.append(image_features.cpu().detach())\n\n        image_features = torch.cat([item for item in image_embed_list], dim=0)\n        text_features = torch.cat([item for item in text_embed_list], dim=0)\n        #lb_image_features = torch.cat([item for item in lb_image_embed_list], dim=0)\n        lb_text_features = torch.cat([item for item in lb_text_embed_list], dim=0)\n        batch_new['image_features'] = image_features\n        text_features = self.text_projection1(text_features.to(d))\n        text_features = self.text_projection2(text_features.to(d))\n        batch_new['text_features'] = text_features\n        \n        #batch_new['lb_image_features'] = lb_image_features\n        batch_new['lb_text_features'] = lb_text_features\n\n        return batch_new\n    def create_masked_text(self, stance):\n            \"\"\"\n            Create a masked text representation for stance towards the target (e.g., LGBTQ movement).\n            The [mask] token is replaced with the stance description.\n            \n            stance: str, the stance label (e.g., \"Support\", \"Deny\", \"Query\", etc.)\n            Returns:\n                A tensor representing the masked stance text\n            \"\"\"\n            # Define the target, e.g., LGBTQ movement, and replace the [mask] token with the stance\n            target = \"LGBTQ movement\"\n            description = f\"The individual expresses their stance towards the {target} as: [mask].\"\n        \n            # Replace the [mask] token with the specific stance label\n            detailed_text = description.replace(\"[mask]\", stance)\n        \n            # Tokenize the final detailed text\n            return clip.tokenize(detailed_text, context_length=20, truncate=True).to(d)\n\n    \n    def compute_CLIP_features_without_proj(self, clip_model, img_input, text_input):\n        image_features = clip_model.visual(img_input.type(clip_model.dtype))\n               \n        x = clip_model.token_embedding(text_input).type(clip_model.dtype)\n        x = x + clip_model.positional_embedding.type(clip_model.dtype)\n        x = x.permute(1, 0, 2)\n        x = clip_model.transformer(x)\n        x = x.permute(1, 0, 2)\n        x = clip_model.ln_final(x).type(clip_model.dtype)\n        text_features = x[torch.arange(x.shape[0]), text_input.argmax(dim=-1)]\n\n        return image_features, text_features\n    def compute_CLIP_features_without_proj2(self, clip_model, text_input):\n        #image_features = clip_model.visual(img_input.type(clip_model.dtype))               \n        # Assuming you use the CLIP tokenizer\n        # Ensure both tensors are on the same device (move the padded tensor to t\n        # Ensure both tensors are on the same device (move the padded tensor to the same device as 'tokens')\n        tokens = clip_model.token_embedding(text_input).type(clip_model.dtype)\n        device = tokens.device  # Get the device of the tokens tensor\n        \n        # Pad the token input to length 77 (or truncate if necessary)\n        padding_length = 77 - tokens.size(1)\n        if padding_length > 0:\n            # Create the padding tensor with the same number of dimensions as 'tokens'\n            padding_tensor = torch.zeros((tokens.size(0), padding_length, tokens.size(2)), dtype=tokens.dtype, device=device)\n            tokens = torch.cat([tokens, padding_tensor], dim=1)\n        else:\n            tokens = tokens[:, :77]  # Truncate if too long\n        \n        # Add positional embeddings\n        x = tokens + clip_model.positional_embedding.type(clip_model.dtype)\n\n\n        x = x.permute(1, 0, 2)\n        x = clip_model.transformer(x)\n        x = x.permute(1, 0, 2)\n        x = clip_model.ln_final(x).type(clip_model.dtype)\n        text_features = x[torch.arange(x.shape[0]), text_input.argmax(dim=-1)]\n\n        return text_features\n\n    def compute_CLIP_features_without_proj3(self, clip_model, text_input):\n            #image_features = clip_model.visual(img_input.type(clip_model.dtype))               \n            # Assuming you use the CLIP tokenizer\n            # Ensure both tensors are on the same device (move the padded tensor to t\n            # Ensure both tensors are on the same device (move the padded tensor to the same device as 'tokens')\n            tokens = clip_model.token_embedding(text_input).type(clip_model.dtype)\n            device = tokens.device  # Get the device of the tokens tensor\n            \n            # Pad the token input to length 77 (or truncate if necessary)\n            padding_length = 77 - tokens.size(1)\n            if padding_length > 0:\n                # Create the padding tensor with the same number of dimensions as 'tokens'\n                padding_tensor = torch.zeros((tokens.size(0), padding_length, tokens.size(2)), dtype=tokens.dtype, device=device)\n                tokens = torch.cat([tokens, padding_tensor], dim=1)\n            else:\n                tokens = tokens[:, :77]  # Truncate if too long\n            \n            # Add positional embeddings\n            x = tokens + clip_model.positional_embedding.type(clip_model.dtype)\n    \n    \n            x = x.permute(1, 0, 2)\n            x = clip_model.transformer(x)\n            x = x.permute(1, 0, 2)\n            x = clip_model.ln_final(x).type(clip_model.dtype)\n            text_features = x[torch.arange(x.shape[0]), text_input.argmax(dim=-1)]\n    \n            return text_features\n\n\ndef load_dataset(split):\n    root_dir = '/kaggle/input/pridemm2025/PrideMM'\n    img_folder = '/kaggle/input/pridemm2025/PrideMM/Images'\n    dataset_name = 'Pride'\n    image_size = 224\n    #label_stance == 'stance'\n    dataset = Custom_Dataset( root_folder=root_dir, dataset=dataset_name, split=split,\n                           image_size=image_size, label = 'stance', fast=True)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.896141Z","iopub.status.idle":"2025-02-18T07:25:33.896560Z","shell.execute_reply":"2025-02-18T07:25:33.896382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass Adapter(nn.Module):\n    def __init__(self, c_in, reduction=4):\n        super(Adapter, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(c_in, c_in // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(c_in // reduction, c_in, bias=False),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n    \nclass Classifier(nn.Module):\n    def __init__(self, feat_dim=768, num_classes=None, dtype=None):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(num_classes, feat_dim, dtype=dtype))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 0, 1e-5).mul_(1e5)\n\n    @property\n    def dtype(self):\n        return self.weight.dtype\n\n    def forward(self, x):\n        raise NotImplementedError\n        \n    def apply_weight(self, weight):\n        self.weight.data = weight.clone()\n\nclass CosineClassifier(Classifier):\n    def __init__(self, feat_dim=None, num_classes=None, dtype=None, scale=30, **kwargs):\n        super().__init__(feat_dim, num_classes, dtype)\n        self.scale = scale\n        self.relu = nn.ReLU()\n        self.drop = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        x = F.normalize(x, dim=-1)\n        weight = F.normalize(self.weight, dim=-1)\n\n        return F.linear(x, weight) * self.scale\n\nclass LinearClassifier(Classifier):\n    def __init__(self, feat_dim=None, num_classes=None, dtype=None, **kwargs):\n        super().__init__(feat_dim, num_classes, dtype)\n        nn.init.kaiming_normal_(self.weight.data)\n        self.bias = nn.Parameter(torch.zeros(num_classes, dtype=dtype))\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\nclass LinearProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, num_layers, drop_probs):\n        super(LinearProjection, self).__init__()\n\n        map_layers = [nn.Linear(input_dim, output_dim),\n                      nn.Dropout(p=drop_probs[0])]\n\n        for _ in range(1, num_layers):\n            map_layers.extend(\n                [nn.ReLU(), nn.Linear(output_dim, output_dim), nn.Dropout(p=drop_probs[0])])\n\n        self.proj = nn.Sequential(*map_layers)\n\n    def __call__(self, *args, **kwargs):\n        return super().__call__(*args, **kwargs)\n\n    def forward(self, x):\n        return self.proj(x)\n    \nclass CLIP_Text(nn.Module):\n    def __init__(self, clip_model):\n        super().__init__()\n        self.token_embedding = clip_model.token_embedding\n        self.positional_embedding = clip_model.positional_embedding\n        self.transformer = clip_model.transformer\n        self.ln_final = clip_model.ln_final\n        self.text_projection = clip_model.text_projection\n        self.dtype = clip_model.dtype\n\n    def forward(self, text):\n        x = self.token_embedding(text).to(self.dtype)  \n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0, 2).type(torch.float32)  \n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  \n        x = self.ln_final(x).to(self.dtype)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection.to(self.dtype)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:33.897883Z","iopub.status.idle":"2025-02-18T07:25:33.898350Z","shell.execute_reply":"2025-02-18T07:25:33.898150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#from models import LinearClassifier, CosineClassifier, LinearProjection, CLIP_Text, Adapter\n\nclass MemeCLIP(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        num_classes=3\n        self.acc = torchmetrics.Accuracy(task='multiclass', num_classes = num_classes)\n        self.auroc = torchmetrics.AUROC(task='multiclass', num_classes = num_classes)\n        self.f1 = torchmetrics.F1Score(task='multiclass', num_classes = num_classes, average='macro')\n        \n        self.clip_model, _ = clip.load(clip_variant, device=d, jit=False)\n        self.clip_model.float()\n        map_dim = 1024\n        pre_output_input_dim = map_dim\n        drop_probs = [0.1, 0.4, 0.2]\n        pre_output_layers = [nn.Dropout(p=drop_probs[1])]\n        pre_output_layers_cat = [nn.Dropout(p=drop_probs[1])]\n        output_input_dim = pre_output_input_dim\n\n        self.classifier = CosineClassifier(feat_dim = output_input_dim, num_classes=num_classes, dtype=self.clip_model.dtype)\n        self.init_head_text_feat()\n        self.text_encoder =  CLIP_Text(self.clip_model)\n        self.img_adapter = Adapter(map_dim, 4).to(self.clip_model.dtype)\n        self.text_adapter = Adapter(map_dim, 4).to(self.clip_model.dtype)\n        self.clip_model.visual.proj = None\n\n        for _, p in self.clip_model.named_parameters():\n            p.requires_grad_(False)\n        \n        for name, param in self.classifier.named_parameters():\n            param.requires_grad_(True)\n        unmapped_dim = 768\n        num_mapping_layers = 1\n        self.image_map = LinearProjection(unmapped_dim, map_dim,\n                                          num_mapping_layers, drop_probs)\n        self.text_map = LinearProjection(unmapped_dim, map_dim,\n                                         num_mapping_layers, drop_probs)\n        \n        self.soft = nn.Softmax(dim=1)\n        num_pre_output_layers = 1\n        if num_pre_output_layers >= 1:\n            pre_output_layers.extend(\n                [nn.Linear(pre_output_input_dim, map_dim), nn.ReLU(), nn.Dropout(p=drop_probs[2])])\n            output_input_dim = map_dim\n\n        for _ in range(1, num_pre_output_layers):\n            pre_output_layers.extend(\n                [nn.Linear(map_dim, map_dim), nn.ReLU(), nn.Dropout(p=drop_probs[2])])\n\n        self.pre_output = nn.Sequential(*pre_output_layers)\n        self.pre_output_cat = nn.Sequential(*pre_output_layers_cat)\n        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n\n    def forward(self, batch):\n        pass\n    \n    def init_head_text_feat(self):\n\n        print(\"Initialize head with text features\")\n        template = \"a photo of a {}.\"\n        class_names = ['Neutral', 'Support', 'Oppose']\n        prompts = [template.format(c.replace(\"_\", \" \")) for c in class_names]\n        \n        prompts = clip.tokenize([p for p in prompts], context_length=77, truncate=True).to(d)\n        text_features = self.clip_model.encode_text(prompts)\n        text_features = F.normalize(text_features, dim=-1)\n        text_features = text_features @ self.clip_model.visual.proj.t()\n        text_features = F.normalize(text_features, dim=-1)\n        self.classifier.apply_weight(text_features)\n\n    def common_step(self, batch):\n\n        image_embeds = batch['image_features']\n        text_embeds = batch['text_features']\n        image_projection = self.image_map(image_embeds)\n        txt_projection = self.text_map(text_embeds)\n        image_features = self.img_adapter(image_projection)\n        text_features = self.text_adapter(txt_projection)\n        ratio=0.2\n        text_features = ratio  * text_features + (1 - ratio ) * txt_projection\n        image_features = ratio  * image_features + (1 - ratio ) * image_projection\n\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        features1 = torch.mul(image_features, text_features) \n\n\n        features_pre_output = self.pre_output_cat(features1)\n        logits = self.classifier(features_pre_output).squeeze(dim=1) \n        preds_proxy = torch.sigmoid(logits)\n        _ , preds = logits.data.max(1)\n\n        output = {}\n        output['loss'] = self.cross_entropy_loss(logits, batch['labels'])\n        output['accuracy'] = self.acc(preds, batch['labels'])\n        output['auroc'] = self.auroc(preds_proxy, batch['labels'])\n        output['f1'] = self.f1(preds, batch['labels'])\n        output['logits'] = logits\n\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        output = self.common_step(batch)\n\n        total_loss = output['loss']\n\n        self.log('train/total_loss', total_loss)\n        self.log('train/loss', output['loss'])\n        self.log('train/accuracy', output['accuracy'])\n        self.log(f'train/auroc', output['auroc'], on_step=False, on_epoch=True, prog_bar=True)\n\n        return total_loss\n\n    # def validation_step(self, batch, batch_idx):\n    #     output = self.common_step(batch)\n\n    #     total_loss = output['loss']\n\n    #     self.log(f'val/total_loss', total_loss)\n    #     self.log(f'val/loss', output['loss'])\n    #     self.log(f'val/accuracy', output['accuracy'], on_step=False, on_epoch=True, prog_bar=True)\n    #     self.log(f'val/auroc', output['auroc'], on_step=False, on_epoch=True, prog_bar=True)\n    #     self.log(f'val/f1', output['f1'], on_step=False, on_epoch=True, prog_bar=True)\n    def validation_step(self, batch, batch_idx):\n        output = self.common_step(batch)\n        #print(output)\n        \n        total_loss = output['loss']\n        \n        # Log the loss and metrics\n        self.log(f'val/total_loss', total_loss)\n        self.log(f'val/loss', output['loss'])\n        self.log(f'val/accuracy', output['accuracy'], on_step=False, on_epoch=True, prog_bar=True)\n        self.log(f'val/auroc', output['auroc'], on_step=False, on_epoch=True, prog_bar=True)\n        self.log(f'val/f1', output['f1'], on_step=False, on_epoch=True, prog_bar=True)\n        \n        # Collect predictions and true labels for classification report\n        predictions = output['logits'].argmax(dim=1)  # Assuming output['logits'] contains model predictions\n        true_labels = batch['labels']  # Assuming 'labels' is the ground truth\n        \n        # Save predictions and true labels for later\n        self.predictions = torch.cat([self.predictions, predictions]) if hasattr(self, 'predictions') else predictions\n        self.true_labels = torch.cat([self.true_labels, true_labels]) if hasattr(self, 'true_labels') else true_labels\n        \n        return total_loss          \n    \n\n    def test_step(self, batch, batch_idx):\n\n        output = self.common_step(batch)\n        self.log(f'test/accuracy', output['accuracy'])\n        self.log(f'test/auroc', output['auroc'])\n        self.log(f'test/f1', output['f1'])\n\n        return output\n\n    def on_train_epoch_end(self):\n        self.acc.reset()\n        self.auroc.reset()\n        self.f1.reset()\n        \n    def on_validation_epoch_end(self):\n        # Calculate and log the classification report at the end of the epoch\n        if hasattr(self, 'predictions') and hasattr(self, 'true_labels'):\n            preds = self.predictions.cpu().numpy()\n            labels = self.true_labels.cpu().numpy()\n            \n            del self.predictions\n            del self.true_labels\n\n    def on_test_epoch_end(self):\n        self.acc.reset()\n        self.auroc.reset()\n        self.f1.reset()\n\n    def configure_optimizers(self):\n        param_dicts = [\n            {\"params\": [p for n, p in self.named_parameters() if p.requires_grad]}\n        ]\n        lr = 1e-4\n        weight_decay = 1e-4\n        optimizer = torch.optim.AdamW(param_dicts, lr=lr, weight_decay=weight_decay)\n\n        return optimizer\n\ndef create_model():\n    model = MemeCLIP()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:34.003317Z","iopub.execute_input":"2025-02-18T07:25:34.003749Z","iopub.status.idle":"2025-02-18T07:25:34.042350Z","shell.execute_reply.started":"2025-02-18T07:25:34.003717Z","shell.execute_reply":"2025-02-18T07:25:34.040578Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-17e84bc303b2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from models import LinearClassifier, CosineClassifier, LinearProjection, CLIP_Text, Adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMemeCLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"],"ename":"NameError","evalue":"name 'pl' is not defined","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"\n\n# Define a GNN-based model\nclass GNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GNNModel, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, output_dim)\n\n    def forward(self, data):\n        # Forward pass through GCN layers\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = torch.relu(x)\n        x = self.conv2(x, edge_index)\n        \n        # Global mean pooling to get a fixed-size output (for classification)\n        x = global_mean_pool(x, data.batch)\n        return x\n\n# Modify the `create_model` function to use the GNNModel\ndef create_model_gnn():\n    # Here, specify the input and output dimensions (e.g., embedding size and number of classes)\n    input_dim = 1024  # Example input dimension (e.g., embedding dimension)\n    hidden_dim = 256\n    output_dim = 4  # Example number of classes for classification\n    \n    model = GNNModel(input_dim, hidden_dim, output_dim)\n    return model\nfrom torch_geometric.data import Data\n\ndef prepare_data_for_gnn(features, edge_index, batch):\n    # Assume features is a tensor of node features, edge_index is the adjacency matrix,\n    # and batch is the batch assignments for graph nodes (typically a tensor of the same length as features).\n    \n    data = Data(x=features, edge_index=edge_index, batch=batch)\n    return data\ndef extract_embeddings_from_gnn(model, data_loader):\n    model.eval()  # Set the model to evaluation mode to avoid dropout\n    embeddings_list = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            # Forward pass through the GNN model to extract embeddings\n            embeddings = model(batch)\n            embeddings_list.append(embeddings)\n    \n    # Concatenate the embeddings into a single tensor\n    all_embeddings = torch.cat(embeddings_list, dim=0)\n    return all_embeddings\nclass GraphDataset(torch.utils.data.Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings        \n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        return self.embeddings[idx]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:34.043198Z","iopub.status.idle":"2025-02-18T07:25:34.043676Z","shell.execute_reply":"2025-02-18T07:25:34.043435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef main():\n    # Set the random seed for reproducibility\n    seed_everything(42, workers=True)\n\n    # Load datasets for training, validation, and testing\n    dataset_train = load_dataset(split='train')\n    dataset_val = load_dataset(split='val')\n    dataset_test = load_dataset(split='test')\n\n    print(\"Number of training examples:\", len(dataset_train))\n    print(\"Number of validation examples:\", len(dataset_val))\n    print(\"Number of test examples:\", len(dataset_test))\n\n    # Define the data loader\n    collator = Custom_Collator()\n    batch_size = 16\n    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True,\n                               collate_fn=collator, num_workers=0)\n    val_loader = DataLoader(dataset_val, batch_size=batch_size, collate_fn=collator, num_workers=0)\n    test_loader = DataLoader(dataset_test, batch_size=batch_size, collate_fn=collator, num_workers=0)\n\n    #Create the model\n    model = create_model()\n\n    #Model checkpoint to save the best model based on validation AUC\n    root_dir = '/kaggle/input/pridemm2025/PrideMM/checkpoints3/'\n    checkpoint_path = os.path.join(root_dir, '')\n    os.makedirs(checkpoint_path, exist_ok=True)\n\n    monitor = \"val/auroc\"\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=checkpoint_path,\n        filename='best_model',  # Save the best model as 'best_model.ckpt'\n        monitor=monitor,\n        mode='max',  # Maximize the AUC score\n        save_top_k=1,  # Save only the best model\n        verbose=True,\n        save_weights_only=True\n    )\n\n    # Define the trainer\n    gpus = [0]  # Select GPU 1 for training\n    max_epochs = 10\n    trainer = Trainer(\n        accelerator='gpu',\n        #devices=gpus,\n        max_epochs=max_epochs,\n        callbacks=[checkpoint_callback],\n        deterministic=False\n    )\n\n    # Train the model\n    #trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n    checkpoint_file = 'dataset/PrideMM/checkpoints3/best_model.ckpt'\n    #checkpoint_file = 'dataset/PrideMM/checkpoints/best_model.ckpt'\n\n    \n\n    # Make sure that create_model is the function that creates the model class instance\n    best_model = MemeCLIP.load_from_checkpoint(checkpoint_file)  # Use the model class, not the function\n\n    #Load the best checkpoint and test the model\n    # best_model_path = checkpoint_callback.best_model_path\n    # print(f\"Best model checkpoint: {best_model_path}\")\n    # best_model = create_model.load_from_checkpoint(best_model_path)\n\n    # Test the best model\n    trainer.test(best_model, dataloaders=test_loader)\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:25:34.044515Z","iopub.status.idle":"2025-02-18T07:25:34.044807Z","shell.execute_reply":"2025-02-18T07:25:34.044689Z"}},"outputs":[],"execution_count":null}]}